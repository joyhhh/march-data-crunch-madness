# Deloitte-March-Data-Crunch-Madness

AN OPTIMIZED MACHINE LEARNING MODEL FOR PREDICTING 2018 MARCH MADNESS MATCH-UP PROBABILITIES

Team name: Fantastic Four

Team member: Beiya Xue; Bojian Zhang; Shu Yang; Yunjian Wei

Introduction

The National Collegiate Athletic Association (NCAA) Men's Basketball Tournament is informally referred to as "March Madness". With 68 college basketball teams competing in a single-elimination tournament, March Madness is played every spring in the US to determine the national championship of the major college basketball teams. Based on Kaggle’s Machine Learning Mania(https://www.kaggle.com/c/mens-machine-learning-competition-2018#evaluation ), competitors of 2018 Deloitte March Data Crunch Madness predict the probability that a team wins any given game in the 2018 Tournament. Logloss is used as evaluation. Extensive historical data to jump-start the modeling process is gived. Competitors are encouraged to incorporate their own sources of data.

Highlights of Our Project

One of the highlight of our project is the new variable, academic performance. We believed that the academic performance of a team will have a positive effect on the winning posibility. Another one is the project's philosophy, less is more. We tried to use as fewer variables as possible to explain the impact of each competition and build as simple model as possible to predict the results of 2018 NCAA(National Collegiate Athletic Association). Besides, we got the lowest logloss, 0.5478. Finally, we won the first prize of this year's competition and compared with the result on Kaggle, we are actually the fourth among more than 900 teams. Check the website (https://www.kaggle.com/c/mens-machine-learning-competition-2018/leaderboard) to see the results of the other analysts on Kaggle.

Why APR?

In this project, we merge NCAA academic progress rate data from website: https://web3.ncaa.org/aprsearch/aprsearch
We have two perspectives to prove that it is a useful variable in our model. First is from statistical point of view. We calculate expected win% of each team and use APR to predict team_win%. We find they are correlated with each other and APR satisfies T-test. In addition, lasso regression select APR as an independent variable (we will talk about our feature selection method later). VIF of APR is small, which means it is a new data dimension, it is independent of original dataset but it makes contribution to our model. 

Team_win%=team_adjoe^11.5/(team_adjoe^11.5+team_adjde^11.5)

Then we do some descriptive analysis about APR.
![image](https://github.com/zhangbojian/march-data-crunch-madness/blob/master/Picture1.jpg)
what graph means: This map shows all the NCAA university locations, their Winning Probability(based on Pythagorean theorem) and their Academic Progress Rate
what insight: Winning Probability have strong connection with Academic Progress Rate.
Implication: university from East coast have more green points, which means those universities with green points have higher Academic Progress Rate; The size of the point means the winning probability of the university, and we can see from the map that most of the green points size are bigger than the size of red points.
![image](https://github.com/zhangbojian/march-data-crunch-madness/blob/master/Picture2.jpg)

What graph means: This map lists all the NCAA champions From 2004-2015, and the map shows their locations, Winning Probability (based on Pythagorean theorem) and their Academic Progress Rate.
What insight: Winning Probability have strong connection with Academic Progress Rate.
Implication: All the NCAA champions have high Academic Progress Rate and also have high Winning Probability, which means APR would be a good predictor to predict the probability of the game.

Feature selection

We assume in the basketball game predictation case, some variables, such as seed, rpi, are extremely important in model, while adding some less important variables would not improve model performance dramatically. In additon, using less variables would form a uniform distribution of wining probabilities, which is flexible to predict cinderella effectively.
![image](https://github.com/zhangbojian/march-data-crunch-madness/blob/master/Picture8.jpg)

In the feature selection part, we choose a variety of approaches: principal component analysis(PCA, KPCA and SPCA), lasso regression and stepwise regression(forward stepwise and backward stepwise). We suppose that we can apply as many methods as possible to our dataset and use a same classifier to judge which method is the best. Here, we use logistic regression and K-fold cross validation to test the effectiveness of feature selection methods. During this process, we also attempt to combine methods to get a better result. However, we find lasso regression individually gives us the best result. More than 30 PCs could explain 90% of data variation, but model accuracy is still worse than that generated by lasso regression, which only selects 16 variables, including APR data. This proves out hypothesis that less is more.
![image](https://github.com/zhangbojian/march-data-crunch-madness/blob/master/Picture7.jpg)

Correlation matrix of selected variables:


![image](https://github.com/zhangbojian/march-data-crunch-madness/blob/master/Picture9.jpg)

Machine learning

We test the performance of 8 models
Logistic Regression, Naïve Bayes, SVM, Gradient Boosting Classifier, Random Forest, Neural Networks, Decision Tree, KNN
and compare testing data MSE of each model
![image](https://github.com/zhangbojian/march-data-crunch-madness/blob/master/Picture5.jpg)

We choose the first four algorithms in our ensemble model, because they have least testing data MSE.
Soft voting ensemble is used together with GridSearch in order to tune the hyperparameters of the individual estimators. In this case, we set range of number of estimators of Gradient boosting from 1000 to 2000.
Naive weighting formula:
![image](https://github.com/zhangbojian/march-data-crunch-madness/blob/master/Picture4.jpg)

Model evaluation

When the ability to predict negative result incorrectly is low, the model has a high possibility to predict team 1 win correctly.
The area between the curve and diagonal line reflects the accuracy of the ensemble model, which is most stable and comparatively more precise.

![image](https://github.com/zhangbojian/march-data-crunch-madness/blob/master/Picture6.jpg)
![image](https://github.com/zhangbojian/march-data-crunch-madness/blob/master/Picture10.jpg)

Conclusion and implementation:

1.Applying as fewer variables as possible to explain the impact of each competition and build a model as simple as possible to predict the results is effective.

2.APR data is a new point of view which makes contribution to prediction.

3.In our model, seed, APR, rpi and shooting percentage are crucial to victory. This result may give a reference to basketball teams to improve their performance.





